---
title: Extrapolating by Inverse Optimization
author:
date: '2019-03-09'
slug: extrapolating-by-inverse-optimization
categories: [Optimization, Bayesian]
tags:
  - Inverse Optimization
  - Linear programming
  - Explainable Models
---



<p><em>Wherein I present a simple example of a prediction problem modeled as inverse optimization, with a brief detour into Bayesian estimation, peppered with snide remarks about data science as it is commonly practiced.</em></p>
<p>Let us imagine a grocer (let’s call her Alice) trying to esimate the impacts of changing the prices of her produce on the purchase behavior of her customers. Following the advice of her data scientist friend she gathers her database of quantities of produce purchased by individual customers under various pricing regimes, and hands him the trove.</p>
<p>The collected data looks something like the following. (In each row the prices and quantities are for <em>all</em> the products Alice sells.)</p>
<table>
<thead>
<tr class="header">
<th align="left">Date</th>
<th align="left">Customer.ID</th>
<th align="left">Price.Vector</th>
<th align="left">Quantity.Vector</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Jan 1</td>
<td align="left">XYZ</td>
<td align="left">[$3.2, $2.5, $1.0, …]</td>
<td align="left">[1 lb, 0.0 lb, 0.0 lb, …]</td>
</tr>
<tr class="even">
<td align="left">Jan 8</td>
<td align="left">ABC</td>
<td align="left">[$2.0, $2.0, $1.5, …]</td>
<td align="left">[0.0 lb, 0.0 lb, 0.5 lb, …]</td>
</tr>
<tr class="odd">
<td align="left">Feb 4</td>
<td align="left">XYZ</td>
<td align="left">[$2.5, $2.0, $1.8, …]</td>
<td align="left">[1.0 lb, 0.0 lb, 0.1 lb, …]</td>
</tr>
<tr class="even">
<td align="left">…</td>
<td align="left">…</td>
<td align="left">…</td>
<td align="left">…</td>
</tr>
</tbody>
</table>
<p>The data scientist friend doesn’t see what the big problem is. He builds deep learning models in his sleep and this seems like a simple regression problem. He could just build a machine to predict the quantities column from the price column and take the afternoon off.</p>
<p>Alas it was not to be; the price vectors that Alice wants to plug into the trained regression model are well outside the ranges of the ones she provided the data scientist for training. She does not see the point of any such modeling if it doesn’t enable extrapolation. She is appalled by the resulting nonsensical predictions, that her friend assures her were produced by algorithms indistinguishable from the ones that in the near future will drive her around town.</p>
<div id="a-physics-of-purchase-behavior" class="section level2">
<h2>A Physics of Purchase Behavior</h2>
<p>Alice decides to take a less shallow approach and write out what she knows about the problem and how it relates to what she wishes to estimate.</p>
<p>She reasons that each of her customers <span class="math inline">\(\smash{c_i}\)</span> has a weekly budget <span class="math inline">\(\smash{b_{i}}\)</span> for produce (<span class="math inline">\(\smash{t}\)</span> stands for the date of the store visit) and a personal produce value vector <span class="math inline">\(\smash{v_i}\)</span>. (She makes the simplifying asssumption that the budget does not vary from week to week.) Each customer may have other individual constraints on the purchases (e.g., at least <span class="math inline">\(3\)</span> lb of fruit, no more than <span class="math inline">\(2\)</span> lb of perishables.)</p>
<p>She hypothesizes that a customer on every weekly visit looks at the produce price vector <span class="math inline">\(\smash{p_t}\)</span>, and mentally runs a simple optimization of the following form to decide what quantities of each item of produce (<span class="math inline">\(x_{it}\)</span>) he will purchase:</p>
<p><span class="math display">\[\begin{aligned}
x_{it} &amp;= \mbox{maximize}_x \; v_i^T x \\
\mbox{subject to} &amp;\;\; \smash{p_t^T} x \leq b_{i} \\
&amp;\; \; A x \leq d_i
\end{aligned}
\]</span></p>
<p>The last constraint captures the notion of the customer-specific constraints mentioned above, where <span class="math inline">\(A\)</span> is assumed known (since its rows define common purchase constraints) while the constraint bounds <span class="math inline">\(d_i\)</span> are not known.</p>
<p>The above optimization problem with a linear objective and constraints is called a <em>Linear Program</em> for which there exist very efficient solvers. For example, here’s a basic implementation of a linear program in R</p>
<pre class="r"><code>library(CVXR)

x &lt;- Variable(2) #only two products 
v &lt;- c(0.1, 0.9)
value &lt;- sum(v * x)

p &lt;- c(3, 2)
constr_budget &lt;- sum(p * x) &lt;= 10

A &lt;- rbind(c(1, 0), c(0, 1))
b &lt;- c(2, 4)
constr_other &lt;- A %*% x &lt;= b

solution &lt;- solve(Problem(Maximize(value), list(constr_budget, constr_other)))
cat(paste0(&#39;Status: &#39;, solution$status))</code></pre>
<pre><code>## Status: optimal</code></pre>
<pre class="r"><code>cat(paste0(round(solution$getValue(x), 2)))</code></pre>
<pre><code>## 0.67 4</code></pre>
<p>That is, if for any customer Alice knew the parameters of <em>their</em> optimization problem (<span class="math inline">\(v_i\)</span> up to a normalizing scalar, <span class="math inline">\(b_i\)</span> and <span class="math inline">\(d_i\)</span>) then predicting the purchase behavior is easy. It just involves solving the above problem for any price vector of interest <span class="math inline">\(p\)</span> to obtain a prediction of that customer’s purchase quantity vector <span class="math inline">\(x_i\)</span>. Note that, albeit esoteric, the optimizer defines a family of regression functions <span class="math inline">\(x_i = Opt(p_t; \theta)\)</span> parameterized by <span class="math inline">\(\theta = (v_i, b_i, d_i)\)</span>.</p>
<p>Therefore the problem of <em>learning</em> the regressor reduces to estimating these parameters from historical prices and their associated quantity vectors. This estimation problem, because it attempts to <em>invert</em> the optimization step, is studied under the heading <em>Inverse Optimization</em>.</p>
</div>
<div id="trading-brain-damage-for-computer-costs" class="section level2">
<h2>Trading Brain Damage for Computer Costs</h2>
<p>Alice codes up the <span class="math inline">\(Opt(.)\)</span> function first in the hope that when she comes up with a way to estimate the parameters, she would be ready with the prediction function. She quickly realizes that a simple approach to estimate the parameters is the following. First select some sensible ranges for the various parameters (knowing the neighborhood well she has a good sense of what reasonable budgets, value vectors )</p>
</div>
