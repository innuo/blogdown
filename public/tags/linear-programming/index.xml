<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Linear programming on Notes To An Earlier Self</title>
    <link>/tags/linear-programming/</link>
    <description>Recent content in Linear programming on Notes To An Earlier Self</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 09 Mar 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/linear-programming/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Extrapolating by Inverse Optimization</title>
      <link>/post/2019/03/09/extrapolating-by-inverse-optimization/</link>
      <pubDate>Sat, 09 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019/03/09/extrapolating-by-inverse-optimization/</guid>
      <description>Wherein I present a simple example of a prediction problem modeled as inverse optimization, with a brief detour into Bayesian estimation, peppered with snide remarks about data science as it is commonly practiced.
Let us imagine a grocer (letâ€™s call her Alice) trying to esimate the impacts of changing the prices of her produce on the purchase behavior of her customers. Following the advice of her data scientist friend she gathers her database of quantities of produce purchased by individual customers under various pricing regimes, and hands him the trove.</description>
    </item>
    
  </channel>
</rss>