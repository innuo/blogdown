<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Selection Bias and the Disappointed Optimizer | Notes To An Earlier Self</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    
  </head>

  <body>
    <nav>
    <ul class="menu">
      
      <li><a href="/">Home</a></li>
      
      <li><a href="/about/">About</a></li>
      
      <li><a href="/categories/">Categories</a></li>
      
      <li><a href="/tags/">Tags</a></li>
      
    </ul>
    <hr/>
    </nav>

<div class="article-meta">
<h1><span class="title">Selection Bias and the Disappointed Optimizer</span></h1>

<h2 class="date">2019/03/25</h2>
</div>

<main>



<p><em>Wherein I introduce and explain the well-studied mechanism behind the pervasive phenomenon in life of things being just a bit worse than expected. I will walk through some of the discussion in two excellent articles, <a href="https://pdfs.semanticscholar.org/ce61/396a4abe1da265f4b5d3f05d11fd206c40f7.pdf">The Optimizer’s Curse: Skepticism and Postdecision Surprise in Decision Analysis</a> by Smith and Winkler and <a href="https://arxiv.org/pdf/1302.7175.pdf">Estimating the Maximum Expected Value: An Analysis of (Nested) Cross Validation and the Maximum Sample Average</a> by van Hasselt.</em></p>
<p>Have you noticed that the checkout lanes you choose at the grocery store tend to be slower than you expected, the products you order off Amazon disappoint, and your second dates are not as exciting as your firsts? If you have, and suspect that the universe is conspiring against you, let me assure you that your suspicions are well-founded. The fundamental laws of probability have indeed been arrayed to scupper your satisfaction. If you haven’t noticed any such conspiracy, I congratulate you on either your consistent practice of Zen Buddhism or on your finely tuned skepticism about things.</p>
<p>The mechanism that may explain this seeming conspiracy, phenomena as diverse as the the replication crisis in social sciences and oil wells producing below expectations, and also superstitions like the Sports Illustrated cover jinx is what Smith and Winker call the <em>Optimizer’s Curse</em>. Let’s look a bit closer at the mechanism (using notation slightly different from Smith and Winkler).</p>
<p>Imagine that you are confronted with a choice between <span class="math inline">\(n\)</span> options, <span class="math inline">\(1, \ldots, n\)</span>. You would of course want to select the option with the highest value to you. Let the <em>true</em> values of these <span class="math inline">\(n\)</span> options to you be the vector <span class="math inline">\(\mu = (\mu_1, \ldots, \mu_n)\)</span>. If these true values were known to you, then you would choose option <span class="math inline">\(m\)</span> where <span class="math inline">\(m = \mbox{argmax}_i\, \mu_i\)</span>.</p>
<p>Instead assume that you only have <em>estimates</em> (<span class="math inline">\(V = (v_1, \ldots, v_n)\)</span>) of the true values. Further assume that the estimates are <em>unbiased</em>. That is, on average each <span class="math inline">\(v_i\)</span> neither overestimates nor underestimates its corresponding true value <span class="math inline">\(\mu_i\)</span>. An obvious strategy now is for you to choose to go with option <span class="math inline">\(k = \mbox{argmax}_i\, v_i\)</span>. That is, you select the option that has the largest estimate of value. After the choice is made, the true value <span class="math inline">\(\mu_k\)</span> is revealed to you. We want to know what the <em>surprise</em> <span class="math inline">\(\mu_k - v_k\)</span> is, on average.</p>
<p>We have <span class="math display">\[\mu_k - v_k \leq \mu_k - v_m \leq \mu_m - v_m\]</span> The first inequality is because <span class="math inline">\(v_k \geq v_m\)</span> (that’s why you chose option <span class="math inline">\(k\)</span>) and the second inequality is because <span class="math inline">\(\mu_m \geq \mu_k\)</span> because <span class="math inline">\(m^{th}\)</span> option is the best (unbeknowst to you.)</p>
<p>Now taking expectations with respect to <span class="math inline">\(V\)</span> we have <span class="math display">\[ E[\mu_k - v_k ] \leq E[\mu_m - v_m] = 0\]</span> The equality of the second term to zero is due to our assumption that the estimates are all (and in particular the <span class="math inline">\(m^{th}\)</span> one is) unbiased. The expected surprise is therefore non-positive. In fact, it is zero only when the probability <span class="math inline">\(P(k = m) = 1\)</span>. That is, if there is a chance that you can make a wrong choice based on the estimates, then your suprises are on average unpleasant.</p>
<div id="disappointment-mitigation" class="section level2">
<h2>Disappointment Mitigation</h2>
<p>If you knew that the true values are being generated from a <em>known</em> distribution <span class="math inline">\(P(\mu)\)</span>, the bias can be <em>completely</em> eliminated by turning the Bayesian crank.</p>
<p>In this case the <em>generative model</em> for our problem can be written as (read <span class="math inline">\(\sim\)</span> “is distributed as”) <span class="math display">\[\begin{aligned}
\mu &amp;\sim P(\mu)\\
v_i &amp;\sim P(.| \mu_i)
\end{aligned}
\]</span></p>
<p>The procedure to follow under this model is to first estimate the posterior means <span class="math inline">\(E[\mu|V] \triangleq (\hat{\mu}_1, \ldots, \hat{\mu}_n)\)</span> and then select the “best” option based on the posterior means, <span class="math inline">\(k = \mbox{argmax}_i\, \hat{\mu}_i = \mbox{argmax}_i\, E[\mu_i|V]\)</span>.</p>
<p>What is the expected surprise under this model? To make things clear I will make it explicit as to what variables the expectations are over.</p>
<p><span class="math display">\[\begin{aligned}
E[\mu_k - \hat{\mu}_k] &amp;\triangleq E_{\mu, V}[\mu_k - \hat{\mu}_k] \\
&amp;= E_V[E_\mu[\mu_k - \hat{\mu}_k|V]]  \\
&amp;= E_V[E_\mu[\mu_k|V] - \hat{\mu}_k] \\
&amp;= E_V[\hat{\mu}_k  - \hat{\mu}_k] = 0
\end{aligned}\]</span></p>
<p>The first line is just the definition of what we mean by average surprise. The second line uses the law of total expectation. The third line uses the fact that since <span class="math inline">\(\hat{\mu}_k\)</span> is a function of <span class="math inline">\(V\)</span>, it is a constant when <span class="math inline">\(V\)</span> is fixed. The last line is from the definition of the posterior estimate <span class="math inline">\(\hat{\mu}_k\)</span>.</p>
<p>Therefore, if you use the posterior estimates of as your expectations of your future value and to select among your options, then your average surprise is zero. The Bayesian posterior expectation <em>shrinks</em> the esimates <span class="math inline">\(v_i\)</span> towards the prior mean of <span class="math inline">\(\mu_i\)</span> which is what makes the procedure unbiased.</p>
<p>Depending of the distribution <span class="math inline">\(P(V|\mu)\)</span> the option that is selected by the Bayesian procedure (i.e., optimizing over <span class="math inline">\(\hat{\mu}\)</span>) can be different from the one by optimizing over <span class="math inline">\(V\)</span>. For example, if we knew that the estimate with the largest maginitude, say <span class="math inline">\(v_j\)</span>, is very noisy, option <span class="math inline">\(j\)</span> may no longer be selected after the shrinkage due to the posterior expectation. An option with a smaller but more accurate estimate may win out.</p>
<p>We know that the Bayesian procedure is unbiased, but does it produce better decisions? In other words, how does <span class="math inline">\(E_{\mu, V}[\mu_k]\)</span> when <span class="math inline">\(k = \mbox{argmax}_i\, \hat{\mu}_i = \mbox{argmax}_i\, E[\mu_i|V]\)</span> compare to <span class="math inline">\(E_{\mu, V}[\mu_j]\)</span> when <span class="math inline">\(j = \mbox{argmax}_i\, v_i\)</span> ? This question is not explicitly addressed by Smith and Winkler, but the following quick argument shows that the Bayesian procedure not only produces unbiased estimates of the value of the decisions you make, but it also <strong>produces better decisions</strong>. Using these definitions of <span class="math inline">\(k\)</span> and <span class="math inline">\(j\)</span> we have <span class="math display">\[ E_{\mu, V}[\mu_k] - E_{\mu, V}[\mu_j] = E_V[E_\mu[\mu_k|V]] - E_V[E_\mu[\mu_j|V]] = E_V[\hat{\mu}_k - \hat{\mu}_j] \geq 0\]</span> The first equality uses the same trick of the law of total expectation, the second equality is from the definition of <span class="math inline">\(\hat{\mu}\)</span> and the inequality is because <span class="math inline">\(k\)</span> is the index which maximizes the posterior mean. Therefore, unless the two procedures <em>always</em> produce the same decisions, on average the Bayesian procedure yields greater value.</p>
<p><strong>A Subtelty</strong>. When I first read the proof of unbiasedness of the Bayesian procedure I was puzzled by it because under a sufficiently flat prior the Bayesian decision procedure can be arbitrarily closely approximated by maximizing over the estimates <span class="math inline">\(v_i\)</span>. So how can the former procedure be unbiased when the latter is not?</p>
<p>I realized that there is a subtle sleight-of-hand in the proof. Note that the fourth line of the proof of unbiasedness of the Bayesian procedure uses the fact that <span class="math inline">\(E_\mu[\mu_k|V] = \hat{\mu}_k\)</span>. This expectation with respect to <span class="math inline">\(\mu\)</span> is being conducted <em>by the universe after the decision</em> , whereas the expectation in <span class="math inline">\(\hat{\mu}_i = E[\mu_i|V]\)</span> to obtain the estimates was conducted <em>by you to make the decision</em>. Therefore, as a quick simulation will verify, if your notion of the prior distribution on <span class="math inline">\(\mu\)</span> differs from the universe’s, the guarantee of unbiasedness does not hold. Generally since our belief about <span class="math inline">\(\mu\)</span> will flatter and wider (indicating our ignorance) than the true prior, the optimizer’s bias is not completely eliminated even with the Bayesian procedure. In this case one could apply Smith and Winkler’s recommendation of a hierarchical prior, which results in the posterior estimates that shrink towards a weighted average of a global prior mean of the values, and the mean of the estimates <span class="math inline">\(\bar{v} = \frac{1}{n}\sum_i v_i\)</span>.</p>
</div>
<div id="train-test-split-and-crossvalidation" class="section level2">
<h2>Train-Test Split and Crossvalidation</h2>
<p>Let us change the setup a bit and postulate that instead of one estimate for each option, we have two independent estimates. That is, for each option <span class="math inline">\(i\)</span> we have two <em>unbiased</em> estimates <span class="math inline">\(v_i^A\)</span> and <span class="math inline">\(v_i^B\)</span> which, conditioned on the true value <span class="math inline">\(\mu_i\)</span>, are independent. What would be the bias of our estimates if we optimize using the first set of estimates <span class="math inline">\(v_i^A\)</span>, but estimate the future values from the second set of estimates <span class="math inline">\(v_i^B\)</span>? That is, first select the option <span class="math inline">\(k = \mbox{argmax}_i\, v_i^A\)</span>, but <em>expect</em> that the value of option <span class="math inline">\(k\)</span> is <span class="math inline">\(v_k^B\)</span>. The expected surprise is given by</p>
<p><span class="math display">\[\begin{align}
E[\mu_k - v_k^B] &amp;= \sum_{i=1}^n E[\mu_k - v_k^B|k = i] P(k=i) \\
&amp;= \sum_{i=1}^n E[\mu_i - v_i^B|v_i^A &gt; v_{j \neq i}^A] P(v_i^A &gt; v_{j \neq i}^A ) \\
&amp;= \sum_{i=1}^n E[\mu_i - v_i^B] P(v_i^A &gt; v_{j \neq i}^A )  = 0\\
\end{align}
\]</span> The first equality is from the law of total expectation, the second uses the definition of <span class="math inline">\(k = i\)</span>, the third uses the independence of the <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> estimates and the assumption that all the <span class="math inline">\(B\)</span> estimates are unbiased.</p>
<p>Now we can relate this whole discussion to how we train machine learning models. Postdecision surprise is hardly a foreign concept to engineers who regularly select the “best” among competing models to release into the wild based on some estimate of their error rate on a training dataset.</p>
</div>

</main>

  <footer>
  <footer>
<br /> <br /> <hr> <div class="just-comments" data-apikey="6e288633-d2ef-48b5-ae19-e03c38c46c67"></div>
</footer>

<script async src="https://just-comments.com/w.js"></script>

<script src="//yihui.name/js/math-code.js"></script>
<script async src="//mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script>

<script async src="//yihui.name/js/center-img.js"></script>

  
  <hr/>
  &copy; Harsha Veeramachaneni 2019. Made using  <a href="https://github.com/rstudio/blogdown">blogdown</a> &amp; <a href="https://gohugo.io/">Hugo</a>
  
  </footer>
  </body>
</html>

